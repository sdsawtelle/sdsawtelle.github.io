{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In which the contents of numerous spam folders gradually erodes my faith in humanity. \n",
    "\n",
    "Week 7 of Andrew Ng's ML course on Coursera introduces the Support Vector Machine algorithm and challenges us to use it for classifying email as spam or ham. Here I use the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to build an SVM spam email classifier in order to learn about the relevant python tools. Part I focuses on the preprocessing of individual emails while Part II focuses on the actual classifier.\n",
    "\n",
    ">## Tools Covered:\n",
    "- `re` for regular expressions to do Natural Language Processing (NLP)\n",
    "- `stopwords` text corpus for removing information-poor words in NLP\n",
    "- `SnowballStemmer` for stemming text in NLP\n",
    "- `BeautifulSoup` for HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import snips as snp  # my snippets\n",
    "snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonya\\Box Sync\\Projects\\course-machine-learning\\hw-wk7-spam-preprocessing\n"
     ]
    }
   ],
   "source": [
    "cd hw-wk7-spam-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at the Data\n",
    "\n",
    "I'm going to pull a set of spam and \"ham\" (non-spam) emails from the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) data sets. This resource has also kindly ham separated emails into easy and hard ham. Each email is stored a a plain text file with the email header information and the email body including HTML markup if applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for accessing all the spam and ham text files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "spampath = join(os.getcwd(), \"spam\")\n",
    "spamfiles = [join(spampath, fname) for fname in listdir(spampath)]\n",
    "\n",
    "hampath = join(os.getcwd(), \"easy_ham\")\n",
    "hamfiles = [join(hampath, fname) for fname in listdir(hampath)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Formatted  File\n",
    "Here is what an email would look like if viewed with proper formatting, like in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From irregulars-admin@tb.tf  Thu Aug 22 14:23:39 2002\n",
      "\n",
      "Return-Path: <irregulars-admin@tb.tf>\n",
      "\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9DAE147C66\n",
      "\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 09:23:38 -0400 (EDT)\n",
      "\n",
      "Received: from phobos [127.0.0.1]\n",
      "\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 14:23:38 +0100 (IST)\n",
      "\n",
      "Received: from web.tb.tf (route-64-131-126-36.telocity.com\n",
      "\n",
      "    [64.131.126.36]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\n",
      "\n",
      "    g7MDGOZ07922 for <zzzz-irr@example.com>; Thu, 22 Aug 2002 14:16:24 +0100\n",
      "\n",
      "Received: from web.tb.tf (localhost.localdomain [127.0.0.1]) by web.tb.tf\n",
      "\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MDP9I16418; Thu, 22 Aug 2002 09:25:09\n",
      "\n",
      "    -0400\n",
      "\n",
      "Received: from red.harvee.home (red [192.168.25.1] (may be forged)) by\n",
      "\n",
      "    web.tb.tf (8.11.6/8.11.6) with ESMTP id g7MDO4I16408 for\n",
      "\n",
      "    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:24:04 -0400\n",
      "\n",
      "Received: from prserv.net (out4.prserv.net [32.97.166.34]) by\n",
      "\n",
      "    red.harvee.home (8.11.6/8.11.6) with ESMTP id g7MDFBD29237 for\n",
      "\n",
      "    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:15:12 -0400\n",
      "\n",
      "Received: from [209.202.248.109]\n",
      "\n",
      "    (slip-32-103-249-10.ma.us.prserv.net[32.103.249.10]) by prserv.net (out4)\n",
      "\n",
      "    with ESMTP id <2002082213150220405qu8jce>; Thu, 22 Aug 2002 13:15:07 +0000\n",
      "\n",
      "MIME-Version: 1.0\n",
      "\n",
      "X-Sender: @ (Unverified)\n",
      "\n",
      "Message-Id: <p04330137b98a941c58a8@[209.202.248.109]>\n",
      "\n",
      "To: undisclosed-recipient: ;\n",
      "\n",
      "From: Monty Solomon <monty@roscom.com>\n",
      "\n",
      "Content-Type: text/plain; charset=\"us-ascii\"\n",
      "\n",
      "Subject: [IRR] Klez: The Virus That  Won't Die\n",
      "\n",
      "Sender: irregulars-admin@tb.tf\n",
      "\n",
      "Errors-To: irregulars-admin@tb.tf\n",
      "\n",
      "X-Beenthere: irregulars@tb.tf\n",
      "\n",
      "X-Mailman-Version: 2.0.6\n",
      "\n",
      "Precedence: bulk\n",
      "\n",
      "List-Help: <mailto:irregulars-request@tb.tf?subject=help>\n",
      "\n",
      "List-Post: <mailto:irregulars@tb.tf>\n",
      "\n",
      "List-Subscribe: <http://tb.tf/mailman/listinfo/irregulars>,\n",
      "\n",
      "    <mailto:irregulars-request@tb.tf?subject=subscribe>\n",
      "\n",
      "List-Id: New home of the TBTF Irregulars mailing list <irregulars.tb.tf>\n",
      "\n",
      "List-Unsubscribe: <http://tb.tf/mailman/listinfo/irregulars>,\n",
      "\n",
      "    <mailto:irregulars-request@tb.tf?subject=unsubscribe>\n",
      "\n",
      "List-Archive: <http://tb.tf/mailman/private/irregulars/>\n",
      "\n",
      "Date: Thu, 22 Aug 2002 09:15:25 -0400\n",
      "\n",
      "\n",
      "\n",
      "Klez: The Virus That Won't Die\n",
      "\n",
      " \n",
      "\n",
      "Already the most prolific virus ever, Klez continues to wreak havoc.\n",
      "\n",
      "\n",
      "\n",
      "Andrew Brandt\n",
      "\n",
      ">>From the September 2002 issue of PC World magazine\n",
      "\n",
      "Posted Thursday, August 01, 2002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Klez worm is approaching its seventh month of wriggling across \n",
      "\n",
      "the Web, making it one of the most persistent viruses ever. And \n",
      "\n",
      "experts warn that it may be a harbinger of new viruses that use a \n",
      "\n",
      "combination of pernicious approaches to go from PC to PC.\n",
      "\n",
      "\n",
      "\n",
      "Antivirus software makers Symantec and McAfee both report more than \n",
      "\n",
      "2000 new infections daily, with no sign of letup at press time. The \n",
      "\n",
      "British security firm MessageLabs estimates that 1 in every 300 \n",
      "\n",
      "e-mail messages holds a variation of the Klez virus, and says that \n",
      "\n",
      "Klez has already surpassed last summer's SirCam as the most prolific \n",
      "\n",
      "virus ever.\n",
      "\n",
      "\n",
      "\n",
      "And some newer Klez variants aren't merely nuisances--they can carry \n",
      "\n",
      "other viruses in them that corrupt your data.\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "http://www.pcworld.com/news/article/0,aid,103259,00.asp\n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Irregulars mailing list\n",
      "\n",
      "Irregulars@tb.tf\n",
      "\n",
      "http://tb.tf/mailman/listinfo/irregulars\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(hamfiles[3]) as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Raw File\n",
    "Now we want to see what the actual strings look like that we will do all our processing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From irregulars-admin@tb.tf  Thu Aug 22 14:23:39 2002\\n', 'Return-Path: <irregulars-admin@tb.tf>\\n', 'Delivered-To: zzzz@localhost.netnoteinc.com\\n', 'Received: from localhost (localhost [127.0.0.1])\\n', '\\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9DAE147C66\\n', '\\tfor <zzzz@localhost>; Thu, 22 Aug 2002 09:23:38 -0400 (EDT)\\n', 'Received: from phobos [127.0.0.1]\\n', '\\tby localhost with IMAP (fetchmail-5.9.0)\\n', '\\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 14:23:38 +0100 (IST)\\n', 'Received: from web.tb.tf (route-64-131-126-36.telocity.com\\n', '    [64.131.126.36]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\\n', '    g7MDGOZ07922 for <zzzz-irr@example.com>; Thu, 22 Aug 2002 14:16:24 +0100\\n', 'Received: from web.tb.tf (localhost.localdomain [127.0.0.1]) by web.tb.tf\\n', '    (8.11.6/8.11.6) with ESMTP id g7MDP9I16418; Thu, 22 Aug 2002 09:25:09\\n', '    -0400\\n', 'Received: from red.harvee.home (red [192.168.25.1] (may be forged)) by\\n', '    web.tb.tf (8.11.6/8.11.6) with ESMTP id g7MDO4I16408 for\\n', '    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:24:04 -0400\\n', 'Received: from prserv.net (out4.prserv.net [32.97.166.34]) by\\n', '    red.harvee.home (8.11.6/8.11.6) with ESMTP id g7MDFBD29237 for\\n', '    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:15:12 -0400\\n', 'Received: from [209.202.248.109]\\n', '    (slip-32-103-249-10.ma.us.prserv.net[32.103.249.10]) by prserv.net (out4)\\n', '    with ESMTP id <2002082213150220405qu8jce>; Thu, 22 Aug 2002 13:15:07 +0000\\n', 'MIME-Version: 1.0\\n', 'X-Sender: @ (Unverified)\\n', 'Message-Id: <p04330137b98a941c58a8@[209.202.248.109]>\\n', 'To: undisclosed-recipient: ;\\n', 'From: Monty Solomon <monty@roscom.com>\\n', 'Content-Type: text/plain; charset=\"us-ascii\"\\n', \"Subject: [IRR] Klez: The Virus That  Won't Die\\n\", 'Sender: irregulars-admin@tb.tf\\n', 'Errors-To: irregulars-admin@tb.tf\\n', 'X-Beenthere: irregulars@tb.tf\\n', 'X-Mailman-Version: 2.0.6\\n', 'Precedence: bulk\\n', 'List-Help: <mailto:irregulars-request@tb.tf?subject=help>\\n', 'List-Post: <mailto:irregulars@tb.tf>\\n', 'List-Subscribe: <http://tb.tf/mailman/listinfo/irregulars>,\\n', '    <mailto:irregulars-request@tb.tf?subject=subscribe>\\n', 'List-Id: New home of the TBTF Irregulars mailing list <irregulars.tb.tf>\\n', 'List-Unsubscribe: <http://tb.tf/mailman/listinfo/irregulars>,\\n', '    <mailto:irregulars-request@tb.tf?subject=unsubscribe>\\n', 'List-Archive: <http://tb.tf/mailman/private/irregulars/>\\n', 'Date: Thu, 22 Aug 2002 09:15:25 -0400\\n', '\\n', \"Klez: The Virus That Won't Die\\n\", ' \\n', 'Already the most prolific virus ever, Klez continues to wreak havoc.\\n', '\\n', 'Andrew Brandt\\n', '>>From the September 2002 issue of PC World magazine\\n', 'Posted Thursday, August 01, 2002\\n', '\\n', '\\n', 'The Klez worm is approaching its seventh month of wriggling across \\n', 'the Web, making it one of the most persistent viruses ever. And \\n', 'experts warn that it may be a harbinger of new viruses that use a \\n', 'combination of pernicious approaches to go from PC to PC.\\n', '\\n', 'Antivirus software makers Symantec and McAfee both report more than \\n', '2000 new infections daily, with no sign of letup at press time. The \\n', 'British security firm MessageLabs estimates that 1 in every 300 \\n', 'e-mail messages holds a variation of the Klez virus, and says that \\n', \"Klez has already surpassed last summer's SirCam as the most prolific \\n\", 'virus ever.\\n', '\\n', \"And some newer Klez variants aren't merely nuisances--they can carry \\n\", 'other viruses in them that corrupt your data.\\n', '\\n', '...\\n', '\\n', 'http://www.pcworld.com/news/article/0,aid,103259,00.asp\\n', '_______________________________________________\\n', 'Irregulars mailing list\\n', 'Irregulars@tb.tf\\n', 'http://tb.tf/mailman/listinfo/irregulars\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(hamfiles[3], \"r\") as myfile:\n",
    "    lines = myfile.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary thoughts: The first line of every file is the most basic header info about the originating address and time the email was sent. There follows a section of keyword-value pairs in the form *keyword: value\\n*. Finally, **the body of each email is separated from the meta info by two newline characters `\\n\\n`.** Note that some of the email bodies contain HTML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Mode with Just the Email Body\n",
    "The first thing I'll try is just doing some NLP on only the email bodies, ignoring all the header info. First write a function that grabs only the body lines of a single email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(fpath):\n",
    "    '''Get email body lines from fpath using first occurence of empty line.'''\n",
    "    with open(fpath, \"r\") as myfile:\n",
    "        try: \n",
    "            lines = myfile.readlines()\n",
    "            idx = lines.index(\"\\n\") # only grabs first instance\n",
    "            return \"\".join(lines[idx:])\n",
    "        except: \n",
    "            print(\"Couldn't decode file %s\" %(fpath,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test it out \n",
    "body= get_body(hamfiles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nKlez: The Virus That Won't Die\\n \\nAlready the most prolific virus ever, Klez continues to wreak havoc.\\n\\nAndrew Brandt\\n>>From the September 2002 issue of PC World magazine\\nPosted Thursday, August 01, 2002\\n\\n\\nThe Klez worm is approaching its seventh month of wriggling across \\nthe Web, making it one of the most persistent viruses ever. And \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from PC to PC.\\n\\nAntivirus software makers Symantec and McAfee both report more than \\n2000 new infections daily, with no sign of letup at press time. The \\nBritish security firm MessageLabs estimates that 1 in every 300 \\ne-mail messages holds a variation of the Klez virus, and says that \\nKlez has already surpassed last summer's SirCam as the most prolific \\nvirus ever.\\n\\nAnd some newer Klez variants aren't merely nuisances--they can carry \\nother viruses in them that corrupt your data.\\n\\n...\\n\\nhttp://www.pcworld.com/news/article/0,aid,103259,00.asp\\n_______________________________________________\\nIrregulars mailing list\\nIrregulars@tb.tf\\nhttp://tb.tf/mailman/listinfo/irregulars\\n\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body  # This is the actual string we are going to be processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Klez: The Virus That Won't Die\n",
      " \n",
      "Already the most prolific virus ever, Klez continues to wreak havoc.\n",
      "\n",
      "Andrew Brandt\n",
      ">>From the September 2002 issue of PC World magazine\n",
      "Posted Thursday, August 01, 2002\n",
      "\n",
      "\n",
      "The Klez worm is approaching its seventh month of wriggling across \n",
      "the Web, making it one of the most persistent viruses ever. And \n",
      "experts warn that it may be a harbinger of new viruses that use a \n",
      "combination of pernicious approaches to go from PC to PC.\n",
      "\n",
      "Antivirus software makers Symantec and McAfee both report more than \n",
      "2000 new infections daily, with no sign of letup at press time. The \n",
      "British security firm MessageLabs estimates that 1 in every 300 \n",
      "e-mail messages holds a variation of the Klez virus, and says that \n",
      "Klez has already surpassed last summer's SirCam as the most prolific \n",
      "virus ever.\n",
      "\n",
      "And some newer Klez variants aren't merely nuisances--they can carry \n",
      "other viruses in them that corrupt your data.\n",
      "\n",
      "...\n",
      "\n",
      "http://www.pcworld.com/news/article/0,aid,103259,00.asp\n",
      "_______________________________________________\n",
      "Irregulars mailing list\n",
      "Irregulars@tb.tf\n",
      "http://tb.tf/mailman/listinfo/irregulars\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(body)  # This is what it would look like properly displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Plan of Attack (order matters)\n",
    "The order of steps in text processing matters a lot if you are trying to extract other features alongside a simple \"Bag of Words\" or \"Word Salad\" model. For instance, if you want to count the number of question marks in the email text then you should probably do it *before* removing all punctuation, but *after* replacing all http addresses (which sometimes contain special characters). Here is a rough outline of all the steps we'll take to get from a messy, marked-up raw text to a delicious word salad:\n",
    "- Strip any HTML tags and leave only text content (also count HTML tags)\n",
    "- Strip all email and web addresses (also count them)\n",
    "- Lowercase everything (also count uppercases)\n",
    "- Strip all dollar signs and numbers(also count them)\n",
    "- Strip away all other punctuation (also count exclamation and question marks)\n",
    "- Standardize all white space to single space (also count newlines and blank lines)\n",
    "- Count the total number of words in our word salad\n",
    "- Strip away all useless \"Stopwords\" (like \"a\", \"the\", \"at\")\n",
    "- Stem all the words down to their root to simplify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML\n",
    "Some of the email bodies contain HTML formatting - the amount of such formatting might be a helpful feature, but the tags themselves we want to strip away. There are also some symbols in HTML documents, like \"<\", that have a reserved shorthand notation since they are otherwise interpreted as markup by the browser. We could write regexes to do all of this HTML processing, but a lovely little package called `beatiful soup` has already done this and provided us with an HTML parser that returns a parsed object. The `get_text()` method lets us pull out everything *except* the markup from this parsed object. You should check out the [official soup docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the email body into HTML elements\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(body, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of HTML elements and specific link elements\n",
    "nhtml = len(soup.find_all())\n",
    "nlinks = len(soup.find_all(\"a\"))\n",
    "\n",
    "# Pull out only the non-markup of the body\n",
    "body = soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding email and web addresses\n",
    "We'll find and count the appearances of email and web addresses, and then replace each one with blank space. A very useful tool for all language processing is the **regular expression**, which is housed in the `re` module of the python standard lib. For more info you can refer to my brief but hopefully edifying [overview of regexes in python](http://sdsawtelle.github.io/blog/output/regular-expressions-in-python.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace and count all URLs \n",
    "regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "body, nhttps = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Replace and count all email addresses\n",
    "regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "body, nemails = regx.subn(repl=\" \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nKlez: The Virus That Won't Die\\n \\nAlready the most prolific virus ever, Klez continues to wreak havoc.\\n\\nAndrew Brandt\\n>>From the September 2002 issue of PC World magazine\\nPosted Thursday, August 01, 2002\\n\\n\\nThe Klez worm is approaching its seventh month of wriggling across \\nthe Web, making it one of the most persistent viruses ever. And \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from PC to PC.\\n\\nAntivirus software makers Symantec and McAfee both report more than \\n2000 new infections daily, with no sign of letup at press time. The \\nBritish security firm MessageLabs estimates that 1 in every 300 \\ne-mail messages holds a variation of the Klez virus, and says that \\nKlez has already surpassed last summer's SirCam as the most prolific \\nvirus ever.\\n\\nAnd some newer Klez variants aren't merely nuisances--they can carry \\nother viruses in them that corrupt your data.\\n\\n...\\n\\n \\n_______________________________________________\\nIrregulars mailing list\\n \\n \\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing and Counting Caps\n",
    "We don't expect whether a word is capitalized or not to reflect some deep difference in tone or meaning, but we *might* expect that an email with a bunch of capitalization reflects a certain tone, so we'll lowercase everything but still count the number of capitalized letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count uppercases\n",
    "nupper = len([charup for charup, char in zip(body, body.lower()) if charup != char])\n",
    "# Lowercase everything\n",
    "body = body.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding numbers, dollar signs, and punctuation\n",
    "We'd like to know the frequency of punctuation which carry certain tones, like exclamation marks, question marks, and dollar signs. Also the frequency of numbers appearing in the email might be a helpful feature. All of these frequencies should be normalized to the number of words in the email to measure the tone or intent of the email rather than its length, but we'll hold off on word-count until we're done with processing. After counting the things we care about, we'll remove all punctuation to get us closer to a pure bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count and replace all numbers (integer and float)\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "body, nnum = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Count and replace all dollar signs\n",
    "regx = re.compile(r\"[$]\")\n",
    "body, ndollar = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "# Count number of special punctuation\n",
    "nexclaim, nquest = body.count(\"!\"), body.count(\"?\")\n",
    "\n",
    "# Remove all other punctuation (dashes replace with space)\n",
    "regx = re.compile(r\"[^\\w\\s_-]+\")  \n",
    "body = regx.sub(repl=\"\", string=body)\n",
    "regx = re.compile(r\"[_-]+\")\n",
    "body = regx.sub(repl=\" \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nklez the virus that wont die\\n \\nalready the most prolific virus ever klez continues to wreak havoc\\n\\nandrew brandt\\nfrom the september   issue of pc world magazine\\nposted thursday august    \\n\\n\\nthe klez worm is approaching its seventh month of wriggling across \\nthe web making it one of the most persistent viruses ever and \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from pc to pc\\n\\nantivirus software makers symantec and mcafee both report more than \\n  new infections daily with no sign of letup at press time the \\nbritish security firm messagelabs estimates that   in every   \\ne mail messages holds a variation of the klez virus and says that \\nklez has already surpassed last summers sircam as the most prolific \\nvirus ever\\n\\nand some newer klez variants arent merely nuisances they can carry \\nother viruses in them that corrupt your data\\n\\n\\n\\n \\n \\nirregulars mailing list\\n \\n \\n\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing White Space and Total Word Count\n",
    "Standardizing white space is an important step, as it makes tokenizing the email into words straightforward, but make sure to do it as a last step since lots of the substitutions we've done have created extra whitespace. Also the number of carriage returns (`\\n` characters) and the number of blank lines (`\\n\\n`) might be predictive so we'll count those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count carriage returs and blank lines\n",
    "nblanks, nnewlines = body.count(\"\\n\\n\"), body.count(\"\\n\")\n",
    "\n",
    "# Make all white space a single space\n",
    "regx = re.compile(r\"\\s+\")\n",
    "body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "# Remove any trailing or leading white space\n",
    "body = body.strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'klez the virus that wont die already the most prolific virus ever klez continues to wreak havoc andrew brandt from the september issue of pc world magazine posted thursday august the klez worm is approaching its seventh month of wriggling across the web making it one of the most persistent viruses ever and experts warn that it may be a harbinger of new viruses that use a combination of pernicious approaches to go from pc to pc antivirus software makers symantec and mcafee both report more than new infections daily with no sign of letup at press time the british security firm messagelabs estimates that in every e mail messages holds a variation of the klez virus and says that klez has already surpassed last summers sircam as the most prolific virus ever and some newer klez variants arent merely nuisances they can carry other viruses in them that corrupt your data irregulars mailing list'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a true bag of words, so now we can get our word count to use in normalizing counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = len(body.split(\" \"))\n",
    "nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words with `nltk`\n",
    "Each email is going to have lots of words which are the \"glue\" of the english language but don't carry much semantic weight in determining the real topic or tone of an email. These are called [Stop Words](https://en.wikipedia.org/wiki/Stop_words) and we will go ahead and strip them out from the start. \n",
    "\n",
    "The Natural Language Tool Kit module (`ntlk`) includes a crap ton of functionality for processing text and also access to some public \"corpora\" such as for stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all useless stopwords\n",
    "bodywords = body.split(\" \")\n",
    "keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "body = \" \".join(keepwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'klez virus wont die already prolific virus ever klez continues wreak havoc andrew brandt september issue pc world magazine posted thursday august klez worm approaching seventh month wriggling across web making one persistent viruses ever experts warn may harbinger new viruses use combination pernicious approaches go pc pc antivirus software makers symantec mcafee report new infections daily sign letup press time british security firm messagelabs estimates every e mail messages holds variation klez virus says klez already surpassed last summers sircam prolific virus ever newer klez variants arent merely nuisances carry viruses corrupt data irregulars mailing list'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with `nltk`\n",
    "This classifier is trying to determine the intent or tone of an email (spam vs. ham) by virtue of the specific words in that email, among other things. We don't expect that a slight variation on the same root word, like \"battery\" versus \"batteries\", carries much difference in intent or tone. Thus when we begin to represent our emails in the feature space of word content, we would do better to replace all the variants of each root with the root itself: this reduces the complexity of emails without really reducing the information about tone or intent. This process is called **stemming** and the `nltk` module has several options for out-of-the-box stemmers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem all words\n",
    "words = body.split(\" \")\n",
    "stemwords = [stemmer.stem(wd) for wd in words]\n",
    "body = \" \".join(stemwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'klez virus wont die alreadi prolif virus ever klez continu wreak havoc andrew brandt septemb issu pc world magazin post thursday august klez worm approach seventh month wriggl across web make one persist virus ever expert warn may harbing new virus use combin pernici approach go pc pc antivirus softwar maker symantec mcafe report new infect daili sign letup press time british secur firm messagelab estim everi e mail messag hold variat klez virus say klez alreadi surpass last summer sircam prolif virus ever newer klez variant arent mere nuisanc carri virus corrupt data irregular mail list'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulate Preprocessing in a Function\n",
    "All of the above steps can be combined to a function that spits out the final processed word salad with the other features of interest that we exracted along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_salad(body):\n",
    "    '''Produce a word salad and some useful features from email body.'''\n",
    "\n",
    "    # Parse HTML extract content only (but count tags)\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    nhtml = len(soup.find_all())\n",
    "    nlinks = len(soup.find_all(\"a\"))\n",
    "    body = soup.get_text()\n",
    "    \n",
    "    # Replace and count all URLs \n",
    "    regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "    body, nhttps = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Replace and count all email addresses\n",
    "    regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "    body, nemails = regx.subn(repl=\" \", string=body)\n",
    "    \n",
    "    # Count uppercases then lowercase everything\n",
    "    nupper = len([charup for charup, char in zip(body, body.lower()) if charup != char])\n",
    "    body = body.lower()\n",
    "    \n",
    "    # Count and replace all numbers (integer and float)\n",
    "    regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "    body, nnum = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Count and replace all dollar signs\n",
    "    regx = re.compile(r\"[$]\")\n",
    "    body, ndollar = regx.subn(repl=\" \", string=body)\n",
    "\n",
    "    # Count number of special punctuation\n",
    "    nexclaim, nquest = body.count(\"!\"), body.count(\"?\")\n",
    "\n",
    "    # Remove all other punctuation (dashes replace with space)\n",
    "    regx = re.compile(r\"[^\\w\\s_-]+\")  \n",
    "    body = regx.sub(repl=\"\", string=body)\n",
    "    regx = re.compile(r\"[_-]+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "    \n",
    "    # Count carriage returs and blank lines\n",
    "    nblanks, nnewlines = body.count(\"\\n\\n\"), body.count(\"\\n\")\n",
    "\n",
    "    # Make all white space a single space\n",
    "    regx = re.compile(r\"\\s+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "    # Remove any trailing or leading white space\n",
    "    body = body.strip(\" \")\n",
    "    \n",
    "    # Get total word count\n",
    "    nwords = len(body.split(\" \"))\n",
    "    freqns = {\"email\": nemails/nwords, \"http\":nhttps/nwords,\n",
    "              \"exclaim\":nexclaim/nwords, \"quest\":nquest/nwords, \n",
    "              \"dollar\":ndollar/nwords, \n",
    "              \"blank\":nblanks/nwords, \"newline\":nnewlines/nwords, \n",
    "              \"html\":nhtml/nwords, \"link\":nlinks/nwords}\n",
    " \n",
    "    # Remove all useless stopwords\n",
    "    bodywords = body.split(\" \")\n",
    "    keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "\n",
    "    # Stem all words\n",
    "    stemwords = [stemmer.stem(wd) for wd in keepwords]\n",
    "    body = \" \".join(stemwords)\n",
    "\n",
    "    return freqns, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blank': 0.02877697841726619,\n",
       " 'dollar': 0.0,\n",
       " 'email': 0.007194244604316547,\n",
       " 'exclaim': 0.05755395683453238,\n",
       " 'html': 0.30935251798561153,\n",
       " 'http': 0.0,\n",
       " 'link': 0.014388489208633094,\n",
       " 'newline': 0.2158273381294964,\n",
       " 'quest': 0.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out our functions\n",
    "body = get_body(spamfiles[179])\n",
    "freqns, body = word_salad(body)\n",
    "freqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello seen nbc cbs cnn even oprah health discoveri actual revers age burn fat without diet exercis proven discoveri even report new england journal medicin forget age diet forev guarante reduc bodi fat build lean muscl without exercis enhac sexual perform remov wrinkl cellulit lower blood pressur improv cholesterol profil improv sleep vision memori restor hair color growth strengthen immun system increas energi cardiac output turn back bodi biolog time clock year month usag free inform get free month suppli hgh click receiv email subscrib opt america mail list remov relat maillist click'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Corpus of Processed Emails\n",
    "Whatever algorithm we ultimately use for classification will require numeric feature vectors, so mapping each word salad to such a vector is the next main task. We'll start by building a corpus that is just a list of fully processed emails, and we'll build alongside it a dataframe of the other features of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails =  [\"email\"]*len(hamfiles + spamfiles)  # Reserve in memory, faster than append\n",
    "fnames = [os.path.split(fpath)[1] for fpath in hamfiles + spamfiles]\n",
    "df = pd.DataFrame(columns = [\"email\", \"http\", \"blank\", \"dollar\", \n",
    "                             \"exclaim\", \"html\", \"link\", \"newline\", \"quest\"], index=fnames)\n",
    "y = [0]*len(hamfiles) + [1]*len(spamfiles)  # Ground truth vector\n",
    "\n",
    "for idx, fpath in enumerate(hamfiles + spamfiles):\n",
    "    body = get_body(fpath)  # Extract only the email body text\n",
    "    freqns, body = word_salad(body)  # All preprocessing\n",
    "    fname = os.path.split(fpath)[1]\n",
    "    emails[idx] = body\n",
    "    df.loc[fname] = freqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>http</th>\n",
       "      <th>blank</th>\n",
       "      <th>dollar</th>\n",
       "      <th>exclaim</th>\n",
       "      <th>html</th>\n",
       "      <th>link</th>\n",
       "      <th>newline</th>\n",
       "      <th>quest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001.ea7e79d3153e7469e7a9c3e0af6a357e</th>\n",
       "      <td>0.00980392</td>\n",
       "      <td>0.00490196</td>\n",
       "      <td>0.0784314</td>\n",
       "      <td>0.0147059</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00490196</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.b3120c4bcbf3101e661161ee7efcb8bf</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003.acfc5ad94bbd27118a0d8685d18c89dd</th>\n",
       "      <td>0.00411523</td>\n",
       "      <td>0.00823045</td>\n",
       "      <td>0.0288066</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00823045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0004.e8d5727378ddde5c3be181df593f1712</th>\n",
       "      <td>0.00645161</td>\n",
       "      <td>0.0129032</td>\n",
       "      <td>0.0451613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005.8c3b9e9c0f3f183ddaf7592a11b99957</th>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0.0483092</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00483092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198068</td>\n",
       "      <td>0.00966184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            email        http      blank  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e  0.00980392  0.00490196  0.0784314   \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf        0.01        0.02       0.06   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd  0.00411523  0.00823045  0.0288066   \n",
       "0004.e8d5727378ddde5c3be181df593f1712  0.00645161   0.0129032  0.0451613   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957  0.00483092  0.00483092  0.0483092   \n",
       "\n",
       "                                          dollar     exclaim        html link  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e  0.0147059           0  0.00490196    0   \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf          0        0.02           0    0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd          0  0.00823045           0    0   \n",
       "0004.e8d5727378ddde5c3be181df593f1712          0           0           0    0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957          0  0.00483092           0    0   \n",
       "\n",
       "                                        newline       quest  \n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e      0.25           0  \n",
       "0002.b3120c4bcbf3101e661161ee7efcb8bf      0.27        0.01  \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd  0.160494           0  \n",
       "0004.e8d5727378ddde5c3be181df593f1712  0.212903           0  \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957  0.198068  0.00966184  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date wed aug chris garrigu messag id cant reproduc error repeat like everi time without fail debug log pick happen pick exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri ftoc pickmsg hit mark hit tkerror syntax error express int note run pick command hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri hit that hit come obvious version nmh im use delta pick version pick nmh compil fuchsia cs mu oz au sun mar ict relev part mh profil delta mhparam pick seq sel list sinc pick command work sequenc actual one that explicit command line search popup one come mh profil get creat kre ps still use version code form day ago havent abl reach cvs repositori today local rout issu think exmh worker mail list'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle these objects for easier access later\n",
    "with open(\"easyham_and_spam_corpus_and_df_and_y.pickle\", \"wb\") as myfile:\n",
    "    pickle.dump([emails, df, y], myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now in position to start mapping emails into a numeric vector space. It turns out there are a lot of ways in which to do this and the proper ML approach would be to search over this space using cross-validation to identify the best approach. This is the subject of Spam Part II. We'll explore different vectorization schemes and feed these vectors into a Support Vector Machine to classify each email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
