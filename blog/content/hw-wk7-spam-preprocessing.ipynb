{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In which the contents of numerous spam folders gradually erodes my faith in humanity. \n",
    "\n",
    "Week 7 of Andrew Ng's ML course on Coursera introduces the Support Vector Machine algorithm and challenges us to use it for classifying email as spam or ham. Here I use the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to build an SVM spam email classifier in order to learn about the relevant python tools. Part I focuses on the preprocessing of individual emails while [Part II](http://sdsawtelle.github.io/blog/output/spam-classification-gridsearch-svm.html) focuses on the actual classifier.\n",
    "\n",
    ">## Tools Covered:\n",
    "- `re` for regular expressions to do Natural Language Processing (NLP)\n",
    "- `stopwords` text corpus for removing information-poor words in NLP\n",
    "- `SnowballStemmer` for stemming text in NLP\n",
    "- `BeautifulSoup` for HTML parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import snips as snp  # my snippets\n",
    "snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonya\\Box Sync\\Projects\\course-machine-learning\\hw-wk7-spam-preprocessing\n"
     ]
    }
   ],
   "source": [
    "cd hw-wk7-spam-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at the Data\n",
    "\n",
    "I'm going to pull a set of spam and \"ham\" (non-spam) emails from the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) data sets. Each email is stored a a plain text file with the email header information and the email body including HTML markup if applicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup for accessing all the spam and ham text files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "spampath = join(os.getcwd(), \"spam\")\n",
    "spamfiles = [join(spampath, fname) for fname in listdir(spampath)]\n",
    "\n",
    "hampath = join(os.getcwd(), \"easy_ham\")\n",
    "hamfiles = [join(hampath, fname) for fname in listdir(hampath)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Formatted  File\n",
    "Here is what an email would look like if viewed with proper formatting, like in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From irregulars-admin@tb.tf  Thu Aug 22 14:23:39 2002\n",
      "\n",
      "Return-Path: <irregulars-admin@tb.tf>\n",
      "\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9DAE147C66\n",
      "\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 09:23:38 -0400 (EDT)\n",
      "\n",
      "Received: from phobos [127.0.0.1]\n",
      "\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 14:23:38 +0100 (IST)\n",
      "\n",
      "Received: from web.tb.tf (route-64-131-126-36.telocity.com\n",
      "\n",
      "    [64.131.126.36]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\n",
      "\n",
      "    g7MDGOZ07922 for <zzzz-irr@example.com>; Thu, 22 Aug 2002 14:16:24 +0100\n",
      "\n",
      "Received: from web.tb.tf (localhost.localdomain [127.0.0.1]) by web.tb.tf\n",
      "\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MDP9I16418; Thu, 22 Aug 2002 09:25:09\n",
      "\n",
      "    -0400\n",
      "\n",
      "Received: from red.harvee.home (red [192.168.25.1] (may be forged)) by\n",
      "\n",
      "    web.tb.tf (8.11.6/8.11.6) with ESMTP id g7MDO4I16408 for\n",
      "\n",
      "    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:24:04 -0400\n",
      "\n",
      "Received: from prserv.net (out4.prserv.net [32.97.166.34]) by\n",
      "\n",
      "    red.harvee.home (8.11.6/8.11.6) with ESMTP id g7MDFBD29237 for\n",
      "\n",
      "    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:15:12 -0400\n",
      "\n",
      "Received: from [209.202.248.109]\n",
      "\n",
      "    (slip-32-103-249-10.ma.us.prserv.net[32.103.249.10]) by prserv.net (out4)\n",
      "\n",
      "    with ESMTP id <2002082213150220405qu8jce>; Thu, 22 Aug 2002 13:15:07 +0000\n",
      "\n",
      "MIME-Version: 1.0\n",
      "\n",
      "X-Sender: @ (Unverified)\n",
      "\n",
      "Message-Id: <p04330137b98a941c58a8@[209.202.248.109]>\n",
      "\n",
      "To: undisclosed-recipient: ;\n",
      "\n",
      "From: Monty Solomon <monty@roscom.com>\n",
      "\n",
      "Content-Type: text/plain; charset=\"us-ascii\"\n",
      "\n",
      "Subject: [IRR] Klez: The Virus That  Won't Die\n",
      "\n",
      "Sender: irregulars-admin@tb.tf\n",
      "\n",
      "Errors-To: irregulars-admin@tb.tf\n",
      "\n",
      "X-Beenthere: irregulars@tb.tf\n",
      "\n",
      "X-Mailman-Version: 2.0.6\n",
      "\n",
      "Precedence: bulk\n",
      "\n",
      "List-Help: <mailto:irregulars-request@tb.tf?subject=help>\n",
      "\n",
      "List-Post: <mailto:irregulars@tb.tf>\n",
      "\n",
      "List-Subscribe: <http://tb.tf/mailman/listinfo/irregulars>,\n",
      "\n",
      "    <mailto:irregulars-request@tb.tf?subject=subscribe>\n",
      "\n",
      "List-Id: New home of the TBTF Irregulars mailing list <irregulars.tb.tf>\n",
      "\n",
      "List-Unsubscribe: <http://tb.tf/mailman/listinfo/irregulars>,\n",
      "\n",
      "    <mailto:irregulars-request@tb.tf?subject=unsubscribe>\n",
      "\n",
      "List-Archive: <http://tb.tf/mailman/private/irregulars/>\n",
      "\n",
      "Date: Thu, 22 Aug 2002 09:15:25 -0400\n",
      "\n",
      "\n",
      "\n",
      "Klez: The Virus That Won't Die\n",
      "\n",
      " \n",
      "\n",
      "Already the most prolific virus ever, Klez continues to wreak havoc.\n",
      "\n",
      "\n",
      "\n",
      "Andrew Brandt\n",
      "\n",
      ">>From the September 2002 issue of PC World magazine\n",
      "\n",
      "Posted Thursday, August 01, 2002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Klez worm is approaching its seventh month of wriggling across \n",
      "\n",
      "the Web, making it one of the most persistent viruses ever. And \n",
      "\n",
      "experts warn that it may be a harbinger of new viruses that use a \n",
      "\n",
      "combination of pernicious approaches to go from PC to PC.\n",
      "\n",
      "\n",
      "\n",
      "Antivirus software makers Symantec and McAfee both report more than \n",
      "\n",
      "2000 new infections daily, with no sign of letup at press time. The \n",
      "\n",
      "British security firm MessageLabs estimates that 1 in every 300 \n",
      "\n",
      "e-mail messages holds a variation of the Klez virus, and says that \n",
      "\n",
      "Klez has already surpassed last summer's SirCam as the most prolific \n",
      "\n",
      "virus ever.\n",
      "\n",
      "\n",
      "\n",
      "And some newer Klez variants aren't merely nuisances--they can carry \n",
      "\n",
      "other viruses in them that corrupt your data.\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "http://www.pcworld.com/news/article/0,aid,103259,00.asp\n",
      "\n",
      "_______________________________________________\n",
      "\n",
      "Irregulars mailing list\n",
      "\n",
      "Irregulars@tb.tf\n",
      "\n",
      "http://tb.tf/mailman/listinfo/irregulars\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(hamfiles[3]) as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Raw File\n",
    "Now we want to see what the actual strings look like that we will do all our processing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From irregulars-admin@tb.tf  Thu Aug 22 14:23:39 2002\\n', 'Return-Path: <irregulars-admin@tb.tf>\\n', 'Delivered-To: zzzz@localhost.netnoteinc.com\\n', 'Received: from localhost (localhost [127.0.0.1])\\n', '\\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id 9DAE147C66\\n', '\\tfor <zzzz@localhost>; Thu, 22 Aug 2002 09:23:38 -0400 (EDT)\\n', 'Received: from phobos [127.0.0.1]\\n', '\\tby localhost with IMAP (fetchmail-5.9.0)\\n', '\\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 14:23:38 +0100 (IST)\\n', 'Received: from web.tb.tf (route-64-131-126-36.telocity.com\\n', '    [64.131.126.36]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id\\n', '    g7MDGOZ07922 for <zzzz-irr@example.com>; Thu, 22 Aug 2002 14:16:24 +0100\\n', 'Received: from web.tb.tf (localhost.localdomain [127.0.0.1]) by web.tb.tf\\n', '    (8.11.6/8.11.6) with ESMTP id g7MDP9I16418; Thu, 22 Aug 2002 09:25:09\\n', '    -0400\\n', 'Received: from red.harvee.home (red [192.168.25.1] (may be forged)) by\\n', '    web.tb.tf (8.11.6/8.11.6) with ESMTP id g7MDO4I16408 for\\n', '    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:24:04 -0400\\n', 'Received: from prserv.net (out4.prserv.net [32.97.166.34]) by\\n', '    red.harvee.home (8.11.6/8.11.6) with ESMTP id g7MDFBD29237 for\\n', '    <irregulars@tb.tf>; Thu, 22 Aug 2002 09:15:12 -0400\\n', 'Received: from [209.202.248.109]\\n', '    (slip-32-103-249-10.ma.us.prserv.net[32.103.249.10]) by prserv.net (out4)\\n', '    with ESMTP id <2002082213150220405qu8jce>; Thu, 22 Aug 2002 13:15:07 +0000\\n', 'MIME-Version: 1.0\\n', 'X-Sender: @ (Unverified)\\n', 'Message-Id: <p04330137b98a941c58a8@[209.202.248.109]>\\n', 'To: undisclosed-recipient: ;\\n', 'From: Monty Solomon <monty@roscom.com>\\n', 'Content-Type: text/plain; charset=\"us-ascii\"\\n', \"Subject: [IRR] Klez: The Virus That  Won't Die\\n\", 'Sender: irregulars-admin@tb.tf\\n', 'Errors-To: irregulars-admin@tb.tf\\n', 'X-Beenthere: irregulars@tb.tf\\n', 'X-Mailman-Version: 2.0.6\\n', 'Precedence: bulk\\n', 'List-Help: <mailto:irregulars-request@tb.tf?subject=help>\\n', 'List-Post: <mailto:irregulars@tb.tf>\\n', 'List-Subscribe: <http://tb.tf/mailman/listinfo/irregulars>,\\n', '    <mailto:irregulars-request@tb.tf?subject=subscribe>\\n', 'List-Id: New home of the TBTF Irregulars mailing list <irregulars.tb.tf>\\n', 'List-Unsubscribe: <http://tb.tf/mailman/listinfo/irregulars>,\\n', '    <mailto:irregulars-request@tb.tf?subject=unsubscribe>\\n', 'List-Archive: <http://tb.tf/mailman/private/irregulars/>\\n', 'Date: Thu, 22 Aug 2002 09:15:25 -0400\\n', '\\n', \"Klez: The Virus That Won't Die\\n\", ' \\n', 'Already the most prolific virus ever, Klez continues to wreak havoc.\\n', '\\n', 'Andrew Brandt\\n', '>>From the September 2002 issue of PC World magazine\\n', 'Posted Thursday, August 01, 2002\\n', '\\n', '\\n', 'The Klez worm is approaching its seventh month of wriggling across \\n', 'the Web, making it one of the most persistent viruses ever. And \\n', 'experts warn that it may be a harbinger of new viruses that use a \\n', 'combination of pernicious approaches to go from PC to PC.\\n', '\\n', 'Antivirus software makers Symantec and McAfee both report more than \\n', '2000 new infections daily, with no sign of letup at press time. The \\n', 'British security firm MessageLabs estimates that 1 in every 300 \\n', 'e-mail messages holds a variation of the Klez virus, and says that \\n', \"Klez has already surpassed last summer's SirCam as the most prolific \\n\", 'virus ever.\\n', '\\n', \"And some newer Klez variants aren't merely nuisances--they can carry \\n\", 'other viruses in them that corrupt your data.\\n', '\\n', '...\\n', '\\n', 'http://www.pcworld.com/news/article/0,aid,103259,00.asp\\n', '_______________________________________________\\n', 'Irregulars mailing list\\n', 'Irregulars@tb.tf\\n', 'http://tb.tf/mailman/listinfo/irregulars\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(hamfiles[3], \"r\") as myfile:\n",
    "    lines = myfile.readlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary thoughts: The first line of every file is the most basic header info about the originating address and time the email was sent. There follows a section of keyword-value pairs in the form `keyword: value\\n`. Finally, **the body of each email is separated from the meta info by two newline characters `\\n\\n`.** Note that some of the email bodies contain HTML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Mode: Email Body as a Bag of Words\n",
    "The first thing I'll try is just doing some NLP on only the email bodies, ignoring all the header info. Ultimately we need to represent each email in some numeric feature space in order to feed it into a classifier algorithm: this is where the *Bag of Words* model comes in. Each email is represented by a vector which quantifies the presence of specific vocab words in that email... that means we need to do some preprocessing.\n",
    "\n",
    "First write a function that grabs only the body lines of a single email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_body(fpath):\n",
    "    '''Get email body lines from fpath using first occurence of empty line.'''\n",
    "    with open(fpath, \"r\") as myfile:\n",
    "        try: \n",
    "            lines = myfile.readlines()\n",
    "            idx = lines.index(\"\\n\") # only grabs first instance\n",
    "            return \"\".join(lines[idx:])\n",
    "        except: \n",
    "            print(\"Couldn't decode file %s\" %(fpath,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test it out \n",
    "body= get_body(hamfiles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nKlez: The Virus That Won't Die\\n \\nAlready the most prolific virus ever, Klez continues to wreak havoc.\\n\\nAndrew Brandt\\n>>From the September 2002 issue of PC World magazine\\nPosted Thursday, August 01, 2002\\n\\n\\nThe Klez worm is approaching its seventh month of wriggling across \\nthe Web, making it one of the most persistent viruses ever. And \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from PC to PC.\\n\\nAntivirus software makers Symantec and McAfee both report more than \\n2000 new infections daily, with no sign of letup at press time. The \\nBritish security firm MessageLabs estimates that 1 in every 300 \\ne-mail messages holds a variation of the Klez virus, and says that \\nKlez has already surpassed last summer's SirCam as the most prolific \\nvirus ever.\\n\\nAnd some newer Klez variants aren't merely nuisances--they can carry \\nother viruses in them that corrupt your data.\\n\\n...\\n\\nhttp://www.pcworld.com/news/article/0,aid,103259,00.asp\\n_______________________________________________\\nIrregulars mailing list\\nIrregulars@tb.tf\\nhttp://tb.tf/mailman/listinfo/irregulars\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body  # This is the actual string we are going to be processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Klez: The Virus That Won't Die\n",
      " \n",
      "Already the most prolific virus ever, Klez continues to wreak havoc.\n",
      "\n",
      "Andrew Brandt\n",
      ">>From the September 2002 issue of PC World magazine\n",
      "Posted Thursday, August 01, 2002\n",
      "\n",
      "\n",
      "The Klez worm is approaching its seventh month of wriggling across \n",
      "the Web, making it one of the most persistent viruses ever. And \n",
      "experts warn that it may be a harbinger of new viruses that use a \n",
      "combination of pernicious approaches to go from PC to PC.\n",
      "\n",
      "Antivirus software makers Symantec and McAfee both report more than \n",
      "2000 new infections daily, with no sign of letup at press time. The \n",
      "British security firm MessageLabs estimates that 1 in every 300 \n",
      "e-mail messages holds a variation of the Klez virus, and says that \n",
      "Klez has already surpassed last summer's SirCam as the most prolific \n",
      "virus ever.\n",
      "\n",
      "And some newer Klez variants aren't merely nuisances--they can carry \n",
      "other viruses in them that corrupt your data.\n",
      "\n",
      "...\n",
      "\n",
      "http://www.pcworld.com/news/article/0,aid,103259,00.asp\n",
      "_______________________________________________\n",
      "Irregulars mailing list\n",
      "Irregulars@tb.tf\n",
      "http://tb.tf/mailman/listinfo/irregulars\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(body)  # This is what it would look like properly displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Plan of Attack (order matters)\n",
    "The order of steps in text processing matters a lot if you are trying to extract other features alongside a simple \"Bag of Words\" or \"Word Salad\" model. For instance, if you want to count the number of question marks in the email text then you should probably do it *before* removing all punctuation, but *after* replacing all http addresses (which sometimes contain special characters). \n",
    "\n",
    "Here is a rough outline of all the steps we'll take to get from a messy, marked-up raw text to a delicious word salad:\n",
    "- Strip any HTML tags and leave only text content (also count HTML tags)\n",
    "- Lowercase everything\n",
    "- Strip all email and web addresses (also count them)\n",
    "- Strip all dollar signs and numbers (also count them)\n",
    "- Strip away all other punctuation (also count exclamation and question marks)\n",
    "- Standardize all white space to single space (also count newlines and blank lines)\n",
    "- Count the total number of words in our word salad\n",
    "- Strip away all useless \"Stopwords\" (like \"a\", \"the\", \"at\")\n",
    "- Stem all the words down to their root to simplify\n",
    "\n",
    "Now, when I say *count* what I really mean is substitute each occurrence with some fixed string: like every web address gets replaced with `\"httpaddr\"`. That way when we ultimately convert each email to a vector of word counts, we'll get a feature that reflects the occurrence of the word \"httpaddr\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML\n",
    "Some of the email bodies contain HTML formatting - the amount of such formatting might be a helpful feature, but the tags themselves we want to strip away, along with some other HTML shorthand. Let's no reinvent the wheel though: a package called `beatiful soup` implements a great HTML parser. The parsed object can be interrogated in various ways as it contains all the information about the HTML structure of the original document. You should check out the [official soup docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the email body into HTML elements\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(body, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull out only the non-markup tex\n",
    "body = soup.get_text()\n",
    "\n",
    "# Count the number of HTML elements and specific link elements\n",
    "nhtml = len(soup.find_all())\n",
    "nlinks = len(soup.find_all(\"a\"))\n",
    "# Sub in special strings for \"counting\"\n",
    "body = body + nhtml*\" htmltag \" + nlinks*\" linktag \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "We don't expect whether a word is capitalized or not to reflect some deep difference in tone or meaning, so we'll lowercase everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lowercase everything\n",
    "body = body.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding email and web addresses\n",
    "We'll find and count the appearances of email and web addresses, and then replace each one with blank space. A very useful tool for all language processing is the **regular expression**, which is housed in the `re` module of the python standard lib. For more info you can refer to my brief but hopefully edifying [overview of regexes in python](http://sdsawtelle.github.io/blog/output/regular-expressions-in-python.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all URLs with special strings\n",
    "regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "body, nhttps = regx.subn(repl=\" httpaddr \", string=body)\n",
    "\n",
    "# Replace all email addresses with special strings\n",
    "regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "body, nemails = regx.subn(repl=\" emailaddr \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nklez  the virus that won t die\\n \\nalready the most prolific virus ever  klez continues to wreak havoc \\n\\nandrew brandt\\n from the september  number  issue of pc world magazine\\nposted thursday  august  number    number \\n\\n\\nthe klez worm is approaching its seventh month of wriggling across \\nthe web  making it one of the most persistent viruses ever  and \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from pc to pc \\n\\nantivirus software makers symantec and mcafee both report more than \\n number  new infections daily  with no sign of letup at press time  the \\nbritish security firm messagelabs estimates that  number  in every  number  \\ne mail messages holds a variation of the klez virus  and says that \\nklez has already surpassed last summer s sircam as the most prolific \\nvirus ever \\n\\nand some newer klez variants aren t merely nuisances they can carry \\nother viruses in them that corrupt your data \\n\\n \\n\\n httpaddr \\n \\nirregulars mailing list\\n emailaddr \\n httpaddr \\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding numbers, dollar signs, and punctuation\n",
    "We'd like to know the frequency of numbers and any punctuation which carries a tones, like exclamation marks, question marks, and dollar signs. After replacing the things we care about, we'll remove all other punctuation to get us closer to a pure bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all numbers with special strings\n",
    "regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "body = regx.sub(repl=\" number \", string=body)\n",
    "\n",
    "# Replace all $, ! and ? with special strings\n",
    "regx = re.compile(r\"[$]\")\n",
    "body = regx.sub(repl=\" dollar \", string=body)\n",
    "regx = re.compile(r\"[!]\")\n",
    "body = regx.sub(repl=\" exclammark \", string=body)\n",
    "regx = re.compile(r\"[?]\")\n",
    "body = regx.sub(repl=\" questmark \", string=body)\n",
    "\n",
    "# Remove all other punctuation (replace with white space)\n",
    "regx = re.compile(r\"([^\\w\\s]+)|([_-]+)\")  \n",
    "body = regx.sub(repl=\" \", string=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nklez  the virus that won t die\\n \\nalready the most prolific virus ever  klez continues to wreak havoc \\n\\nandrew brandt\\n from the september  number  issue of pc world magazine\\nposted thursday  august  number    number \\n\\n\\nthe klez worm is approaching its seventh month of wriggling across \\nthe web  making it one of the most persistent viruses ever  and \\nexperts warn that it may be a harbinger of new viruses that use a \\ncombination of pernicious approaches to go from pc to pc \\n\\nantivirus software makers symantec and mcafee both report more than \\n number  new infections daily  with no sign of letup at press time  the \\nbritish security firm messagelabs estimates that  number  in every  number  \\ne mail messages holds a variation of the klez virus  and says that \\nklez has already surpassed last summer s sircam as the most prolific \\nvirus ever \\n\\nand some newer klez variants aren t merely nuisances they can carry \\nother viruses in them that corrupt your data \\n\\n \\n\\n httpaddr \\n \\nirregulars mailing list\\n emailaddr \\n httpaddr \\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing White Space and Total Word Count\n",
    "Standardizing white space is an important step, as it makes tokenizing the email into words straightforward. I do this as a last step since some of my preprocessing creates extra whitespace. The number of carriage returns (`\\n`) and the number of blank lines (`\\n\\n`) might be predictive so we'll replace those with special strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all newlines and blanklines with special strings\n",
    "regx = re.compile(r\"\\n\")\n",
    "body = regx.sub(repl=\" newline \", string=body)\n",
    "regx = re.compile(r\"\\n\\n\")\n",
    "body = regx.sub(repl=\" blankline \", string=body)\n",
    "\n",
    "# Make all white space a single space\n",
    "regx = re.compile(r\"\\s+\")\n",
    "body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "# Remove any trailing or leading white space\n",
    "body = body.strip(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newline klez the virus that won t die newline newline already the most prolific virus ever klez continues to wreak havoc newline newline andrew brandt newline from the september number issue of pc world magazine newline posted thursday august number number newline newline newline the klez worm is approaching its seventh month of wriggling across newline the web making it one of the most persistent viruses ever and newline experts warn that it may be a harbinger of new viruses that use a newline combination of pernicious approaches to go from pc to pc newline newline antivirus software makers symantec and mcafee both report more than newline number new infections daily with no sign of letup at press time the newline british security firm messagelabs estimates that number in every number newline e mail messages holds a variation of the klez virus and says that newline klez has already surpassed last summer s sircam as the most prolific newline virus ever newline newline and some newer klez variants aren t merely nuisances they can carry newline other viruses in them that corrupt your data newline newline newline newline httpaddr newline newline irregulars mailing list newline emailaddr newline httpaddr newline newline'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a true bag of words, so now we can get our total word count to use in normalizing things like number of exclamation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwords = len(body.split(\" \"))\n",
    "nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words with `nltk`\n",
    "Each email is going to have lots of common words which are the \"glue\" of the english language but don't carry much information. These are called [Stop Words](https://en.wikipedia.org/wiki/Stop_words) and we will go ahead and strip them out from the start. \n",
    "\n",
    "The Natural Language Tool Kit module (`ntlk`) defines a ton of functionality for processing text, including a corpus of these so-called stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all useless stopwords\n",
    "bodywords = body.split(\" \")\n",
    "keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "body = \" \".join(keepwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newline klez virus die newline newline already prolific virus ever klez continues wreak havoc newline newline andrew brandt newline september number issue pc world magazine newline posted thursday august number number newline newline newline klez worm approaching seventh month wriggling across newline web making one persistent viruses ever newline experts warn may harbinger new viruses use newline combination pernicious approaches go pc pc newline newline antivirus software makers symantec mcafee report newline number new infections daily sign letup press time newline british security firm messagelabs estimates number every number newline e mail messages holds variation klez virus says newline klez already surpassed last summer sircam prolific newline virus ever newline newline newer klez variants merely nuisances carry newline viruses corrupt data newline newline newline newline httpaddr newline newline irregulars mailing list newline emailaddr newline httpaddr newline newline'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with `nltk`\n",
    "This classifier is trying to determine the intent or tone of an email (spam vs. ham) by virtue of the specific words in that email. But we don't expect that a variation on the same root word, like \"battery\" versus \"batteries\", carries much difference in intent or tone. **In \"stemming\" we replace all the variants of each root with the root itself: this reduces the complexity of the email representation without really reducing the information.** The `nltk` module has several options for out-of-the-box stemmers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem all words\n",
    "words = body.split(\" \")\n",
    "stemwords = [stemmer.stem(wd) for wd in words]\n",
    "body = \" \".join(stemwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newlin klez virus die newlin newlin alreadi prolif virus ever klez continu wreak havoc newlin newlin andrew brandt newlin septemb number issu pc world magazin newlin post thursday august number number newlin newlin newlin klez worm approach seventh month wriggl across newlin web make one persist virus ever newlin expert warn may harbing new virus use newlin combin pernici approach go pc pc newlin newlin antivirus softwar maker symantec mcafe report newlin number new infect daili sign letup press time newlin british secur firm messagelab estim number everi number newlin e mail messag hold variat klez virus say newlin klez alreadi surpass last summer sircam prolif newlin virus ever newlin newlin newer klez variant mere nuisanc carri newlin virus corrupt data newlin newlin newlin newlin httpaddr newlin newlin irregular mail list newlin emailaddr newlin httpaddr newlin newlin'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encapsulate Preprocessing in a Function\n",
    "All of the above steps can into a function that spits out the final processed word salad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_salad(body):\n",
    "    '''Produce a word salad from email body.'''    \n",
    "    # Parse HTML extract content only (but count tags)\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    body = soup.get_text()\n",
    "    \n",
    "    # Pull out only the non-markup tex\n",
    "    body = soup.get_text()\n",
    "\n",
    "    # Count the number of HTML elements and specific link elements\n",
    "    nhtml = len(soup.find_all())\n",
    "    nlinks = len(soup.find_all(\"a\"))\n",
    "    # Sub in special strings for \"counting\"\n",
    "    body = body + nhtml*\" htmltag \" + nlinks*\" linktag \"\n",
    "    \n",
    "    # lowercase everything\n",
    "    body = body.lower()\n",
    "    \n",
    "    # Replace all URLs with special strings\n",
    "    regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "    body, nhttps = regx.subn(repl=\" httpaddr \", string=body)\n",
    "\n",
    "    # Replace all email addresses with special strings\n",
    "    regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "    body, nemails = regx.subn(repl=\" emailaddr \", string=body)\n",
    "    \n",
    "    # Replace all numbers with special strings\n",
    "    regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "    body = regx.sub(repl=\" number \", string=body)\n",
    "\n",
    "    # Replace all $, ! and ? with special strings\n",
    "    regx = re.compile(r\"[$]\")\n",
    "    body = regx.sub(repl=\" dollar \", string=body)\n",
    "    regx = re.compile(r\"[!]\")\n",
    "    body = regx.sub(repl=\" exclammark \", string=body)\n",
    "    regx = re.compile(r\"[?]\")\n",
    "    body = regx.sub(repl=\" questmark \", string=body)\n",
    "\n",
    "    # Remove all other punctuation (replace with white space)\n",
    "    regx = re.compile(r\"([^\\w\\s]+)|([_-]+)\")  \n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "    \n",
    "    # Replace all newlines and blanklines with special strings\n",
    "    regx = re.compile(r\"\\n\")\n",
    "    body = regx.sub(repl=\" newline \", string=body)\n",
    "    regx = re.compile(r\"\\n\\n\")\n",
    "    body = regx.sub(repl=\" blankline \", string=body)\n",
    "\n",
    "    # Make all white space a single space\n",
    "    regx = re.compile(r\"\\s+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "    # Remove any trailing or leading white space\n",
    "    body = body.strip(\" \")\n",
    " \n",
    "    # Remove all useless stopwords\n",
    "    bodywords = body.split(\" \")\n",
    "    keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "\n",
    "    # Stem all words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemwords = [stemmer.stem(wd) for wd in keepwords]\n",
    "    body = \" \".join(stemwords)\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newlin newlin newlin hello emailaddr newlin newlin seen nbc cbs cnn even oprah exclammark health newlin discoveri actual revers age burn fat newlin without diet exercis exclammark proven discoveri even newlin report new england journal medicin newlin forget age diet forev exclammark guarante exclammark newlin newlin reduc bodi fat build lean muscl without exercis exclammark newlin enhac sexual perform newlin remov wrinkl cellulit newlin lower blood pressur improv cholesterol profil newlin improv sleep vision memori newlin restor hair color growth newlin strengthen immun system newlin increas energi cardiac output newlin turn back bodi biolog time clock number number year newlin number month usag exclammark exclammark exclammark newlin free inform get free newlin number month suppli hgh click newlin receiv email subscrib newlin opt america mail list newlin remov relat maillist newlin newlin click newlin newlin newlin htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag htmltag linktag linktag'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out our function\n",
    "body = get_body(spamfiles[179])\n",
    "processed = word_salad(body)\n",
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Our Corpus of  Emails\n",
    "Whatever algorithm we ultimately use for classification will require numeric feature vectors, so mapping each word salad to such a vector is the next main task. We'll start by building a corpus of the raw email bodies, that is just a list of email body strings, and we'll build alongside it a list of the processed email body strings for our own inspection. These lists can later be fed into algorithms for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails_raw =  [\"email\"]*len(hamfiles + spamfiles)  # Reserve in memory, faster than append\n",
    "emails_processed =  [\"email\"]*len(hamfiles + spamfiles)  # Reserve in memory, faster than append\n",
    "y = [0]*len(hamfiles) + [1]*len(spamfiles)  # Ground truth vector\n",
    "\n",
    "for idx, fpath in enumerate(hamfiles + spamfiles):\n",
    "    body = get_body(fpath)  # Extract only the email body text\n",
    "    emails_raw[idx] = body\n",
    "    processed = word_salad(body)  # All preprocessing\n",
    "    emails_processed[idx] = processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle these objects for easier access later\n",
    "with open(\"easyham_and_spam_corpus_raw_and_processed_and_y.pickle\", \"wb\") as myfile:\n",
    "    pickle.dump([emails_raw, emails_processed, y], myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now in position to start mapping emails into a numeric vector space. It turns out there are a lot of ways in which to do this and the proper ML approach would be to search over this space using cross-validation to identify the best approach. This is the subject of Spam Part II. We'll explore different vectorization schemes and feed these vectors into a Support Vector Machine to classify each email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
