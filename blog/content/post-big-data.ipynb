{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In which I learn how *not* to set my computer on fire by trying to read a 3 GB CSV file.\n",
    "\n",
    "Here I'll talk about some tricks for working with larger-than-memory data sets in python using our good friend `pandas` as well as a standard lib module `sqlite3` (for interfacing local (on your machine) databases). If you have a laptop with 4 or even 8 GB of RAM you could find yourself in this position with even just a normal Kaggle competition (I certainly did, hence this post).\n",
    "\n",
    ">Tools covered:\n",
    "- Pandas `nrows` and `skiprows` kwargs for `read_csv`\n",
    "- Pandas `dtypes` and `astype` for \"compressing\" features\n",
    "- `sqlite` relational database software and super basic `SQL` syntax\n",
    "- `sqlite3` Python bindings for sqlite\n",
    "- Pandas `read_sql_query` for pulling from a database to a dataframe\n",
    "- Pandas `to_sql` for writing a dataframe to a database\n",
    "\n",
    "# The Example Data Set\n",
    "For the purposes of this post I'm going to use a laughably *small* `.csv` just to to get my point across. The data is a set of bank customers with some demographic information, stored in `data.csv` which has about 15 rows (pretend I said 15 million). Ok, first things first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peek Inside a Large CSV\n",
    "If you are starting with a really huge file often there is no piece of software that will let you just straight up load it- this is frustrating because obviously before you start to *do* anything with the file you want an idea of what it contains! Luckily you can use pandas to take a quick look at the beginning or inside of the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>acct_type</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310001</td>\n",
       "      <td>Primmer</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>savings</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>442877</td>\n",
       "      <td>Conner</td>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>checking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id last_name  age sex acct_type  active\n",
       "0  310001   Primmer   39   F   savings       0\n",
       "1  442877    Conner   31   M  checking       0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first few rows of the CSV file\n",
    "pd.read_csv(\"data.csv\", nrows=2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102001</td>\n",
       "      <td>Proust</td>\n",
       "      <td>72</td>\n",
       "      <td>M</td>\n",
       "      <td>savings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279300</td>\n",
       "      <td>Piper</td>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>checking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1   2  3         4  5\n",
       "0  102001  Proust  72  M   savings  1\n",
       "1  279300   Piper  31  M  checking  1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at the middle of the CSV file\n",
    "pd.read_csv(\"data.csv\", nrows=2, skiprows=7, header=None).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in A Large CSV Chunk-by-Chunk\n",
    "Pandas provides a convenient handle for reading in chunks of a large CSV file one at time. By setting the `chunksize` kwarg for `read_csv` you will get a generator for these chunks, each one being a dataframe with the same header (column names). This can sometimes let you preprocess each chunk down to a smaller footprint by e.g. dropping columns or changing datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primmer\n",
      "Sater\n",
      "Smith\n"
     ]
    }
   ],
   "source": [
    "# Read in the data chunk by chunk\n",
    "for chunk in pd.read_csv(\"data.csv\", chunksize=4):\n",
    "    print(chunk.loc[0, \"last_name\"])  # Each chunk gives you a dataframe with the same header :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Being Smart About Data Types\n",
    "When reading in a .csv pandas does a good job inferring appropriate datatypes for each column, but it is not memory-optimized and with a large file this can cost you. \n",
    "\n",
    "A good approach is to read in a very large but manageable chunk of the data frame, check what `dtypes` pandas has defaulted to, and then inspect the columns of the dataframe to see if you can improve on the defaults. It's also informative to take a look at the dataframes `memory_usage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>acct_type</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310001</td>\n",
       "      <td>Primmer</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>savings</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>442877</td>\n",
       "      <td>Conner</td>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>checking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>541890</td>\n",
       "      <td>Desalle</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>checking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333365</td>\n",
       "      <td>Paulo</td>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>savings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>442877</td>\n",
       "      <td>Sater</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>checking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id last_name  age  sex acct_type  active\n",
       "0  310001   Primmer   39    F   savings       0\n",
       "1  442877    Conner   31    M  checking       0\n",
       "2  541890   Desalle   19  NaN  checking       1\n",
       "3  333365     Paulo   39    F   savings       1\n",
       "4  442877     Sater   22    M  checking       1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", nrows=5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            int64\n",
       "last_name    object\n",
       "age           int64\n",
       "sex          object\n",
       "acct_type    object\n",
       "active        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        80\n",
       "id           40\n",
       "last_name    40\n",
       "age          40\n",
       "sex          40\n",
       "acct_type    40\n",
       "active       40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas calls strings \"object\" datatypes, more info on pandas data types is [here](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/03-data-types-and-format.html). And here is the [list of allowed numpy data types](https://docs.scipy.org/doc/numpy/user/basics.types.html).]\n",
    "\n",
    "A great example here is that we believe \"active\" is going to be just binary 1/0 values, but pandas wants to be safe so it has used `np.int32` instead of the smaller `np.int8`. Also, since `np.int32` can represent positive and negative integers up to 2 billion, we don't really need `np.int64` for the \"age\", or even the \"id\" column.\n",
    "\n",
    "You can cast columns into a paricular datatype during the csv read by passing in a dictionary of `{colname: dtype}`, or you can process the dataframe chunks after the read with the `astype` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cast during the csv read\n",
    "df = pd.read_csv(\"data.csv\", nrows=5, dtype={\"active\": np.int8})  \n",
    "\n",
    "# ...or cast after reading \n",
    "df[\"age\"] = df[\"age\"].astype(np.int16)\n",
    "df[\"id\"] = df[\"id\"].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            int16\n",
       "last_name    object\n",
       "age           int16\n",
       "sex          object\n",
       "acct_type    object\n",
       "active         int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        80\n",
       "id           10\n",
       "last_name    40\n",
       "age          10\n",
       "sex          40\n",
       "acct_type    40\n",
       "active        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a actually a lot more than can be done in terms of processing data *during* pandas `read_csv`. Check out [this SO answer](http://stackoverflow.com/questions/20095983/specify-correct-dtypes-using-pandas-read-csv) and also the [official `read_csv` docs](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) for more info. You can do things like: \n",
    "- parse columns to date time (`parse_dates` kwarg)\n",
    "- apply arbitrary conversion functions to each column (`converters` kwarg)\n",
    "- specify values that should be recognized as NaN (`na_values` kwarg)\n",
    "- specify reading a selection of columns (`usecols` kwarg)\n",
    "\n",
    "## NaNs Spoil Everything\n",
    "A lot of the above settings for `read_csv` will not work with columns that have NaN. The pandas representation for missing data, `np.nan`, is actually a float64 data type, so if you have e.g. a binary column of 0s and 1s then even one NaN will be enough to force it to be typed float64 rather than int8. In these cases you can let pandas use a large `dtype` when reading in a chunk and then do your own processing of the column to impute NaNs, followed by `astype` to convert to get a smaller data type.\n",
    "\n",
    "The `na_values` kwarg during the csv read can convert things like the string \"NaN\" into a `np.nan` object, which can save you from having a mixed-data column of e.g. numeric with string \"NaN\".  \n",
    "\n",
    "## Compressing Categorical Variables\n",
    "In the above example we have the \"sex\" category which ultimately is binary, but here is represented by strings. To save on RAM you can transform categoricals into an integer mapping, and then `astype` the column to `np.int8` or `np.int16`. \n",
    "\n",
    "A neat tool for doing categorical encoding is the scikit learn `LabelEncoder` class. You \"fit\" this encoder by giving it data which contains all the possible unique values that category will take on. A word of warning though, **LabelEncoder breaks on `np.nan` so first impute these to e.g. the string \"NaN\" or some such.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'M', 'NAN'], dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast during the csv read\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_sex = LabelEncoder()\n",
    "\n",
    "# Impute the NaNs in the sex column, let's make them their own category:\n",
    "df.fillna(value={\"sex\": \"NAN\"}, inplace=True)\n",
    "df[\"sex\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'M', 'NAN'], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_sex.fit(df[\"sex\"].unique())  # Fit the encoder\n",
    "le_sex.classes_  # The fit results in two classes - M and F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the encoder has been fitted you can use it to \"transform\" other sets of this data into integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(le_sex.transform(df[\"sex\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    1\n",
       "Name: sex, dtype: int8"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sex categories into integers and then change type\n",
    "df[\"sex\"] = le_sex.transform(df[\"sex\"])\n",
    "df[\"sex\"] = df[\"sex\"].astype(np.int8)\n",
    "df[\"sex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes all of this bending over backwords with chunks and data type still isn't sufficient to let you work with the full data set in RAM. In such cases you can consider using a local SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does a Database Help With Memory?\n",
    "Say you have a `.csv` file and you want to extract from it only the rows with user ID having a certain value. The only way to do that is to look through the entire file line by line (read through it's bytestream): a `.csv.` file is simply not a data structure you can interact with easily. [I found [this SO](http://stackoverflow.com/a/17108741/4639070) answer enlightening regarding files just being byte streams.]\n",
    "\n",
    "If the file is small enough we can read the `.csv` into a data structure that *is* nice to interact with, like a `pandas` `DataFrame`, which lives in your working memory (RAM). But if the data set is very large then you instead need a data structure that lives on your disk rather than in RAM but is designed to still be easy and fast to interact with. The answer to this riddle is a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Local Databases with `sqlite3`\n",
    "**Sqlite is a lean relational database software that runs locally on your PC** (rather than a remote server). Each database is stored as a single file on disk. Sqlite understands most SQL language syntax although some things are omitted and other things added. **The `sqlite3` module for in the Python Standard Library provides the functionality for working with Sqlite databases within Python.** \n",
    "\n",
    "Preliminarily, **a database is a collection of `tables` each of which is like a spreadsheet with rows and columns.** Fundamentally you must be able to *Create* (put new data into a table), *Read* (pull data out of a table), *Update* (modify existing data in a table), and *Delete* (drop data from a table).\n",
    "\n",
    "[If you want to physically browse through a sqlite DB there is software for that - [DB Browser for Sqlite](http://sqlitebrowser.org/) is a good one.]\n",
    "\n",
    "## Create sqlite Database and Connect to it with  Python\n",
    "**Within Python databases will be represented through `connection` type objects** which you create. To actually interact with the database requires a **`cursor` object to which we can pass SQL commands.** This is super easy to set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connex = sqlite3.connect(\"all_your_base.db\")  # Opens file if exists, else creates file\n",
    "cur = connex.cursor()  # This object lets us actually send messages to our DB and receive results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read .csv into Database by Chunks\n",
    "With a big file don't try to import the whole thing into your DB, use an intermediate tool that chunks the CSV up and then load each chunk into the DB. There are several tools for this, but I'll stick with pandas `df.to_sql` method.\n",
    "\n",
    "Remember how a database is a collection of tables? You must specify both the `connection` object which identifies which database you are connected to, but also the identifier for the particular table within the database that you want to write to. If your database is empty it will create a table from the name you pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primmer\n",
      "Sater\n",
      "Smith\n"
     ]
    }
   ],
   "source": [
    "for chunk in pd.read_csv(\"data.csv\", chunksize=4):\n",
    "    chunk.to_sql(name=\"data\", con=connex, if_exists=\"append\", index=False)  #\"name\" is name of table you're accessing in DB\n",
    "    print(chunk.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One \"gotcha\" with `to_sql` is that by default it will write the pandas dataframe index to the database as it's own column. Since the index is *usually* just incidental, remember to set the kwarg `index=False` to prevent this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Briefest Possible Introduction to SQL\n",
    "You probably want to know *something* about the SQL language at this point, before we start throwing queries around willy nilly. Since the internet overfloweth with [useful SQL tutorials](http://www.python-course.eu/sql_python.php) I won't bother with that here - I'll just give the flavor of it.\n",
    "\n",
    "Here is a SQL command example for creating a `table` called \"person\" which houses general information about people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"CREATE TABLE person (id INTEGER PRIMARY KEY, first_name TEXT, last_name TEXT, age INTEGER);\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL commands are usually pretty easy to interpet. We are creating a table called `person` which has several columns, each of which has it's type specified. Some things of note:\n",
    "- SQL commands are terminated by a semicolon\n",
    "- The convention is to UPPERCASE the words which are SQL keywords (as opposed to e.g. a column name)\n",
    "- The `PRIMARY KEY` keyword says that this column is the basis of the index for looking things up in this table. That means finding things by \"id\" will be very fast. You can make additional indices based on different columns also.\n",
    "\n",
    "Let's look at another example that fetches the first_name and last_name columns from the \"person\" table of the database. (I mean, we just created it so it's obviously empty, but pretend it's not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT first_name, last_name FROM person;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more: let's add a new column to this table to hold what town the person lives in, notice we can specify a default value to be applied to all existing entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1a9ee483d50>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"ALTER TABLE person ADD COLUMN town TEXT DEFAULT 'North Pole';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you know SQL, so easy!  OK you don't *know* SQL, but you can probably recognize the cadence of the queries, and you won't be intimidated by the weird mixture of UPPER and lower case..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually Doing Stuff With Your Database!\n",
    "## Query Your DB with execute\n",
    "When you send a SQL query command via your cursor the results are returned as a generator which you can work with either directly or through some `cursor` methods. The `fetchone` method acts like `next()` to grab the next value, while the `fetchall` method pulls values from the generator until it is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x215f04aa6c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT DISTINCT sex FROM data;\"  # A SQL command to get unique values from the sex column \n",
    "cur.execute(sql)  # The cursor gives this command to our DB, the results are now available in cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F',)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.fetchone()  # Pop the first result of the query like `next()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M',), (None,)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.fetchall()  # Pop the remaining results of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('F',)\n",
      "('M',)\n",
      "(None,)\n"
     ]
    }
   ],
   "source": [
    "# Execute returns a generator you can loop through.\n",
    "for result in cur.execute(sql):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull DataFrames from your DB with `pandas`\n",
    "`Pandas` actually has a lot of built in functionality that makes it easy to shuttle data between DataFrames and SQL databases - you just pass it the `cursor` object and the command string. This is soooo convenient - I heart pandas :) \n",
    "\n",
    "Here is a simple row-selection operation that reads the results right into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494724,102001)\n"
     ]
    }
   ],
   "source": [
    "ids = [494724, 102001]\n",
    "ids = [str(id) for id in ids] \n",
    "str_matching = \"(\" + \",\".join(ids) + \")\"  # Construct the string of SQL language\n",
    "print(str_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM data WHERE id IN (494724,102001);\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT * FROM data WHERE id IN \" + str_matching + \";\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>acct_type</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494724</td>\n",
       "      <td>Mellon</td>\n",
       "      <td>30</td>\n",
       "      <td>F</td>\n",
       "      <td>checking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102001</td>\n",
       "      <td>Proust</td>\n",
       "      <td>72</td>\n",
       "      <td>M</td>\n",
       "      <td>savings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id last_name  age sex acct_type  active\n",
       "0  494724    Mellon   30   F  checking       0\n",
       "1  102001    Proust   72   M   savings       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql_query(sql, connex)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DataFrames to your DB with `pandas`\n",
    "Since you can pull a dataframe out of a DB, you would expect to be able to push one to it also. Let's make some new entries in a new dataframe and then write them to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(columns=df.columns,\n",
    "                      data={\"id\": [200100, 109333], \n",
    "                      \"last_name\": [\"Caster\", \"Spenny\"], \n",
    "                      \"age\": [81, 62],\n",
    "                      \"sex\": [\"F\", \"F\"],\n",
    "                      \"acct_type\": [np.nan, \"savings\"],\n",
    "                      \"active\": [1, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>acct_type</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200100</td>\n",
       "      <td>Caster</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109333</td>\n",
       "      <td>Spenny</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>savings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id last_name  age sex acct_type  active\n",
       "0  200100    Caster   81   F       NaN       1\n",
       "1  109333    Spenny   62   F   savings       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new.to_sql(name=\"data\", con=connex, if_exists=\"append\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kwarg `if_exists` tells pandas what to do if the table in the database already exists. We can double check the results of our push by reading out unique last names from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Primmer',),\n",
       " ('Conner',),\n",
       " ('Desalle',),\n",
       " ('Paulo',),\n",
       " ('Sater',),\n",
       " ('Mellon',),\n",
       " ('Proust',),\n",
       " ('Piper',),\n",
       " ('Smith',),\n",
       " ('Brimbo',),\n",
       " ('Chamberlin',),\n",
       " ('Yollinski',),\n",
       " ('Caster',),\n",
       " ('Spenny',)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT DISTINCT last_name FROM data;\"  # A SQL command to get unique values from the sex column \n",
    "cur.execute(sql)  # The cursor gives this command to our DB, the results are now available in cur\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Speed With a New Index\n",
    "If you know you will be pulling records according to the value of a certain column(s) very frequently, make a new *index* for your database on that column. You can see the time difference even with a very tiny database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql2 = \"CREATE INDEX id_idx ON data (id);\"\n",
    "cur.execute(sql2)\n",
    "connex.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Changes and Disconnecting!\n",
    "Whenever we would like to save changes that we have made to the database we must commit them, and once we are finished with the database we need to close it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connex.commit()\n",
    "connex.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're Being Naughty With Parameter Passing\n",
    "I've just been using vanilla python string formatting to construct my SQL queries, and then passing the string to the database. This is actually kinda frowned upon - you're instead supposed to use a special string formatting convention where variable values to be filled in are placeheld by `?`. Ideally you would do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM data WHERE ncodpers IN (?, ?);\", con=connex, params=(100, 101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would fill in the question marks with whatever you provided for the `params` kwarg. **This doesn't work with the sqlite driver for some reason so instead you have to do all your string formatting outside the `read_sql_query` call.** Just thought you should know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Links\n",
    "\n",
    "- http://www.datacarpentry.org/python-ecology-lesson/08-working-with-sql\n",
    "- https://plot.ly/python/big-data-analytics-with-pandas-and-sqlite/\n",
    "- http://sebastianraschka.com/Articles/2013_sqlite_database.html#results-and-conclusions\n",
    "- http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html#creating-unique-indexes\n",
    "\n",
    "\n",
    "[Tutorial examples with `sqlite3` in Python [here](http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html), [here](http://pythoncentral.io/introduction-to-sqlite-in-python/).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
