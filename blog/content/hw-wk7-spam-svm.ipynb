{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In which I help you decide if cash4u@freestuff.net is legit or not.\n",
    "\n",
    "Week 7 of Andrew Ng's ML course on Coursera introduces the Support Vector Machine algorithm and challenges us to use it for classifying email as spam or ham. Here I use the [SpamAssassin public corpus](https://spamassassin.apache.org/publiccorpus/) to build an SVM spam email classifier in order to learn about the relevant python tools. [Part I](http://sdsawtelle.github.io/blog/output/spam-classification-text-processing.html) focused on the preprocessing of individual emails, but now I'm going to actually do some machine learning. \n",
    "\n",
    ">## Tools Covered:\n",
    "- `CountVectorizer` for mapping text data to numeric word occurrence vectors\n",
    "- `tfidfTransformer` for normalizing word occurrence vectors \n",
    "- `Pipeline` for chaining together transformer (preprocessing, feature extraction) and estimator steps\n",
    "- `GridSearchCV` for optimizing over the metaparameters of an estimator or pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sonya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import snips as snp  # my snippets\n",
    "snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sonya\\Box Sync\\Projects\\course-machine-learning\\hw-wk7-spam-svm\n"
     ]
    }
   ],
   "source": [
    "cd hw-wk7-spam-svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Look at the Data\n",
    "In [Part I](http://sdsawtelle.github.io/blog/output/spam-classification-text-processing.html) of this post I created a corpus of raw email bodies, and fully processed email bodies, together with the vector of ground truth for these emails ($y=0$ for ham). The processed emails have had all stop words removed and all words stemmed to their root. \n",
    "\n",
    "I'll first read in these pickled objects and give a quick preview of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"easyham_and_spam_corpus_raw_and_processed_and_y.pickle\", \"rb\") as myfile:\n",
    "    emails_raw, emails_processed, y = pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Date:        Wed, 21 Aug 2002 10:54:46 -0500\\n    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\\n    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\\n\\n\\n  | I can\\'t reproduce this error.\\n\\nFor me it is very repeatable... (like every time, without fail).\\n\\nThis is the debug log of the pick happening ...\\n\\n18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\\n18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\\n18:19:04 Ftoc_PickMsgs {{1 hit}}\\n18:19:04 Marking 1 hits\\n18:19:04 tkerror: syntax error in expression \"int ...\\n\\nNote, if I run the pick command by hand ...\\n\\ndelta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\\n1 hit\\n\\nThat\\'s where the \"1 hit\" comes from (obviously).  The version of nmh I\\'m\\nusing is ...\\n\\ndelta$ pick -version\\npick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\\n\\nAnd the relevant part of my .mh_profile ...\\n\\ndelta$ mhparam pick\\n-seq sel -list\\n\\n\\nSince the pick command works, the sequence (actually, both of them, the\\none that\\'s explicit on the command line, from the search popup, and the\\none that comes from .mh_profile) do get created.\\n\\nkre\\n\\nps: this is still using the version of the code form a day ago, I haven\\'t\\nbeen able to reach the cvs repository today (local routing issue I think).\\n\\n\\n\\n_______________________________________________\\nExmh-workers mailing list\\nExmh-workers@redhat.com\\nhttps://listman.redhat.com/mailman/listinfo/exmh-workers\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date wed aug chris garrigu messag id cant reproduc error repeat like everi time without fail debug log pick happen pick exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri ftoc pickmsg hit mark hit tkerror syntax error express int note run pick command hand delta pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri hit that hit come obvious version nmh im use delta pick version pick nmh compil fuchsia cs mu oz au sun mar ict relev part mh profil delta mhparam pick seq sel list sinc pick command work sequenc actual one that explicit command line search popup one come mh profil get creat kre ps still use version code form day ago havent abl reach cvs repositori today local rout issu think exmh worker mail list'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Text to Vectors\n",
    "Any classification algorithm we might want to use for this task is going to expect numeric input features, so we can't just feed it our email strings. Instead we'll map each email to a vector whose elements reflect the presence of specific words in that email. For instance, we could map to a vector $x$ where $x_1 = $ (# of times the word 'money' appears), $x_2 = $ (# of times the word 'stop' appears) etc. This mapping constitutes a \"Bag of Words\" approach because no information about the relative position of words in the email is retained. The words that we count to construct the elements of this vector constitute our **Vocab List**, in the above our vocab list would be ['money', 'stop']. **Each vocab word leads to a feature that reflects the presence of that word in an email.**\n",
    "\n",
    "These numeric vectors can reflect either the binary presence of the words or the number of occurrences of the words. Let me quote directly from a [wonderful sklearn tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) on the topic of occurrence count features:\n",
    "\n",
    "> Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for **\"Term Frequencies\"**\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "This downscaling is called **tf–idf for “Term Frequency times Inverse Document Frequency”**\n",
    "\n",
    "The sklearn docs also have [more info on tf-idf](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) in the User Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a \"Vocab List\" of Selected Words\n",
    "If we make a list of all the words from all the emails then it will have a very large number of unique elements, but some of those words don't appear very often and thus won't be very useful for us. Instead its common to make your Vocab List from only the significantly common words. Let's explore what our vocab list might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatlist = [word for email in emails_processed for word in email.split(\" \")]  # A flat list of all words from all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count number of appearances of each word and store as dictionary\n",
    "import collections\n",
    "counts_dict = collections.Counter(flatlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get a sense of the distribution over word-frequencies. The word \"number\" is going to be vastly more common than any other word and definitely any word that only appears once or twice in the whole email corpus is not going to help us much, so we'll ignore those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAFlCAYAAAC+xHyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1edc214d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 4])\n",
    "snp.labs(\"Occurrence Count Value\", \"Frequency of Occurrence Count Value\", \"Frequency of Occurrence Count :)\")\n",
    "plt.hist([num for num in counts_dict.values() if num > 50 and num < 12000], bins=1000)\n",
    "plt.xlim([50, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a sense of what our vocab list would be like if we only used words who appeared a hundred or more times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist = [key for key in counts_dict if counts_dict[key] > 100]\n",
    "len(vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion', 'ad', 'togeth', 'propos', 'generat', 'stuff', 'edit']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablist[10:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This criteria of demanding a frequency of at least 100 appearances would give us a vocab list of 865 words to use to define our feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the Emails\n",
    "**Given the corpus of processed emails and Vocab List of selected words, we want to map each email to a numeric vector that reflects the occurrence (or frequency) of different vocab words.** The tool in sklearn that helps us do this is `CountVectorizer`. It is a class that tokenizes input text and converts it into a numeric vector. Let's do an example using the vocab list we generated above and assuming we want our vectors to reflect actual word count, rather than binary presence of the word (if you want binary, then specify kwarg `binary=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3046x877 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 150787 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify our vocab list, and use processed email bodies\n",
    "vectorizer = CountVectorizer(vocabulary=vocablist)  \n",
    "X = vectorizer.transform(emails_processed)\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[30, 30] # The number of times the 30th word appears in the 30th email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Preprocessing Function\n",
    "Left to its own devices `CountVectorizer` will intelligently tokenize your email to extract individual words to the best of its ability. For more control you can pass it your own preprocessing function which should take in a document (raw email body in this case) and return a processed one. The function we defined in [Part I](http://sdsawtelle.github.io/blog/output/spam-classification-text-processing.html), `word_salad()` is exactly this kind of preprocessor, and so we could feed our raw email body strings to `CountVectorizer` instead of our processed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_salad(body):\n",
    "    '''Produce a word salad from email body.'''    \n",
    "    # Parse HTML extract content only (but count tags)\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    body = soup.get_text()\n",
    "    \n",
    "    # Pull out only the non-markup tex\n",
    "    body = soup.get_text()\n",
    "\n",
    "    # Count the number of HTML elements and specific link elements\n",
    "    nhtml = len(soup.find_all())\n",
    "    nlinks = len(soup.find_all(\"a\"))\n",
    "    # Sub in special strings for \"counting\"\n",
    "    body = body + nhtml*\" htmltag \" + nlinks*\" linktag \"\n",
    "    \n",
    "    # lowercase everything\n",
    "    body = body.lower()\n",
    "    \n",
    "    # Replace all URLs with special strings\n",
    "    regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "    body, nhttps = regx.subn(repl=\" httpaddr \", string=body)\n",
    "\n",
    "    # Replace all email addresses with special strings\n",
    "    regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "    body, nemails = regx.subn(repl=\" emailaddr \", string=body)\n",
    "    \n",
    "    # Replace all numbers with special strings\n",
    "    regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "    body = regx.sub(repl=\" number \", string=body)\n",
    "\n",
    "    # Replace all $, ! and ? with special strings\n",
    "    regx = re.compile(r\"[$]\")\n",
    "    body = regx.sub(repl=\" dollar \", string=body)\n",
    "    regx = re.compile(r\"[!]\")\n",
    "    body = regx.sub(repl=\" exclammark \", string=body)\n",
    "    regx = re.compile(r\"[?]\")\n",
    "    body = regx.sub(repl=\" questmark \", string=body)\n",
    "\n",
    "    # Remove all other punctuation (replace with white space)\n",
    "    regx = re.compile(r\"([^\\w\\s]+)|([_-]+)\")  \n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "    \n",
    "    # Replace all newlines and blanklines with special strings\n",
    "    regx = re.compile(r\"\\n\")\n",
    "    body = regx.sub(repl=\" newline \", string=body)\n",
    "    regx = re.compile(r\"\\n\\n\")\n",
    "    body = regx.sub(repl=\" blankline \", string=body)\n",
    "\n",
    "    # Make all white space a single space\n",
    "    regx = re.compile(r\"\\s+\")\n",
    "    body = regx.sub(repl=\" \", string=body)\n",
    "\n",
    "    # Remove any trailing or leading white space\n",
    "    body = body.strip(\" \")\n",
    " \n",
    "    # Remove all useless stopwords\n",
    "    bodywords = body.split(\" \")\n",
    "    keepwords = [word for word in bodywords if word not in stopwords.words('english')]\n",
    "\n",
    "    # Stem all words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemwords = [stemmer.stem(wd) for wd in keepwords]\n",
    "    body = \" \".join(stemwords)\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3046x877 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 150787 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify our custom preprocessor and use raw email bodies\n",
    "vectorizer = CountVectorizer(preprocessor=word_salad, vocabulary=vocablist)\n",
    "X = vectorizer.transform(emails_raw)\n",
    "X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Vocab List Generation\n",
    "`CountVectorizer` is capable of creating a vocab list for you automatically, based on the criterion of *document* frequency (the number or proportion of documents that a word appears in). You can set the max and min frequency or proportion using kwargs `min_df` and `max_df` (a float is interpreted as proportion of documents). To create the vocab list you must first `fit` the vectorizer with your corpus, then it can be used to `transform` the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=['date wed aug chris garrigu messag id cant reproduc error repeat like everi time without fail debug log pick happen pick exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri ftoc pickmsg hit m...umber conveni time return call certain happi remov address list click follow link send blank email'],\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.03,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let vectorizer automatically build a vocab list\n",
    "vectorizer = CountVectorizer(min_df=0.03, max_df=1.0)  \n",
    "vectorizer.fit(emails_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vectorizer` object now has a vocab list attribut that can be accessed in various ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address', 'ago', 'agre', 'allow', 'almost', 'along', 'alreadi']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[10:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Chunks of More than One Word (n-grams)\n",
    "With `CountVectorizer` we have to option to include not just individual words in our vocab list but also groups of $n$ contiguous words (n-grams)? This would take us beyond the \"bag of words\" model because now the position of words in an email matters. The `ngram_range` kwarg takes a tuple that specifies the smallest group size and largest group size that will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=['date wed aug chris garrigu messag id cant reproduc error repeat like everi time without fail debug log pick happen pick exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri exec pick inbox list lbrace lbrace subject ftp rbrace rbrace sequenc mercuri ftoc pickmsg hit m...umber conveni time return call certain happi remov address list click follow link send blank email'],\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.03,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.03, max_df=1.0, ngram_range=(1, 2)) \n",
    "vectorizer.fit(emails_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified a maximum n-gram size of two, we should now see single words and word pairs in our fitted vocab list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['either', 'els', 'email', 'email sponsor', 'encod', 'end', 'end pgp']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[143:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranforming Count Occurrence Vectors to TF or TF-IDF\n",
    "As we said, occurrence count for a word has a lot of drawbacks, but it can be turned into a more robust feature with `TfidTransformer`. You can ask this tool for just the term-frequencies, or the \"Term Frequency times Inverse Document Frequency” (tf-idf). It expects to be passed a matrix of occurrence counts, like that which you get from `CountVectorizer`, and it will return your the tf or tf-idf transformed matrix of frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080669648511006734"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X)\n",
    "X_tf = tf_transformer.transform(X)\n",
    "\n",
    "X_tf[30, 30]  # term frequency of 30th vocab word in the 30th email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29097091996720964"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=True).fit(X)\n",
    "X_tfidf = tfidf_transformer.transform(X)\n",
    "\n",
    "X_tfidf[30, 30]  # term freqn - inverse doc freqn of 30th vocab word in the 30th email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Over Preprocessers with `Pipeline` and `GridSearchCV`\n",
    "Clearly some uncertainty still remains in what is the best way to proceed with vectorization:\n",
    "\n",
    "- What words should be included in this vocab list?\n",
    "- Should the numeric feature be binary occurrence, tf, or tf-idf?\n",
    "- Should we use only words in our vocab list, or also include groups of $n$ contiguous words (n-grams)?\n",
    "\n",
    "All of the above questions have to do with *preprocessing* of the data into the numeric vectors rather than with the SVM algorithm itself. To compare the performance of an algorithm under different preprocessing options in sklearn we can use a `Pipeline`: a chain of sklearn objects which transforms (or fits) your data sequentially. Once the pipeline is created, `GridSearchCV` can be used to vary parameters of any step in the pipeline. \n",
    "\n",
    "A pipeline has its own `fit` method that results in the full sequence of transformations being executed ending with a fit of the final estimator in the pipeline. I'll be using a support vector machine as my final estimator, you can take a look at another of my posts if you want to learn more about [SVMs and Grid Search in python](http://sdsawtelle.github.io/blog/output/week7-andrew-ng-machine-learning-with-python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You initialize a pipeline by providing it with every object in the pipeline, each of which you give a nickname to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define component objects of our pipeline then create it!\n",
    "objs = [(\"vect\", CountVectorizer()), \n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"svm\", SVC(kernel=\"rbf\"))]\n",
    "pipe = Pipeline(objs)\n",
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct the grid search on the parameter space we'll need to pass `GridSearchCV` the pipeline we just created, as well as a specification of what parameters of the different objects we will be varying, and over what range. Within the vectorizer we will try different minimum frequencies as our vocab list cutoff and n-grams of different sizes. Within the tf-idf transformer we'll try including vs. not including inverse document frequency. Within the SVM we will want to optimize over the inverse regularization, C, as well as the spread parameter of the guassian kernel, gamma. \n",
    "\n",
    "First I'll try a smaller subset of these parameter values, to avoid running a long computation. To refer to a specific parameters of a specific step in the pipeline you use `<estimator nickname>__<parameter>` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify parameters of the pipeline and their ranges for grid search\n",
    "params = {\n",
    "    'vect__min_df': np.linspace(0.005, 0.05, 5),\n",
    "    'vect__ngram_range': ((1, 1),(1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'svm__C': np.logspace(-1, 2, 10),\n",
    "    'svm__gamma': np.logspace(-1, 1, 10),\n",
    "}\n",
    "\n",
    "# Construct our grid search object\n",
    "search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search object is induced to actually perform the search by the `fit` method, the results can then be inspected. When running the algorithm at each point in your parameter space grid this guy will do 3-fold cross-validation (you can change the fold number with kwarg `cv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__min_df': array([ 0.005  ,  0.01625,  0.0275 ,  0.03875,  0.05   ]), 'vect__ngram_range': ((1, 1), (1, 2)), 'svm__gamma': array([  0.1    ,   0.16681,   0.27826,   0.46416,   0.77426,   1.29155,\n",
       "         2.15443,   3.59381,   5.99484,  10.     ]), 'tfidf__use_idf': (True, False), 'svm__C': array([   0.1    ,    0.21544,    0.46416,    1.     ,    2.15443,\n",
       "          4.64159,   10.     ,   21.54435,   46.41589,  100.     ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(emails_processed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values:\n",
      "('vect__min_df', 0.0050000000000000001)\n",
      "('vect__ngram_range', (1, 2))\n",
      "('svm__gamma', 2.1544346900318834)\n",
      "('tfidf__use_idf', False)\n",
      "('svm__C', 2.1544346900318834)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter values:\")\n",
    "for param in search.best_params_.items():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score using best parameter values: 0.983585029547\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Score using best parameter values:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring metric here is the default scoring method for the final estimator: in this case it's classification accuracy. We're doing pretty well, at 98% of emails correctly classified. It seems that using a large number of vocab words (`min_df=0.02`) and using the idf weighting (`use_idf=True`) gave us the best performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
